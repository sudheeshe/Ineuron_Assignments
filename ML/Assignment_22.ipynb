{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8b445d7",
   "metadata": {},
   "source": [
    "### 1. Is there any way to combine five different models that have all been trained on the same training data and have all achieved 95 percent precision? If so, how can you go about doing it? If not, what is the reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb7b74a",
   "metadata": {},
   "source": [
    "Ensemble modeling is a process where multiple diverse models are created to predict an outcome, either by using many different modeling algorithms or using different training data sets or same data set for training. The ensemble model then aggregates the prediction of each base model and results in once final prediction for the unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919bd8fb",
   "metadata": {},
   "source": [
    "### 2. What's the difference between hard voting classifiers and soft voting classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e524b876",
   "metadata": {},
   "source": [
    "In classification problems, there are two types of voting: hard voting and soft voting. Hard voting entails picking the prediction with the highest number of votes, whereas soft voting entails combining the probabilities of each prediction in each model and picking the prediction with the highest total probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ac4de6",
   "metadata": {},
   "source": [
    "### 3. Is it possible to distribute a bagging ensemble's training through several servers to speed up the process? Pasting ensembles, boosting ensembles, Random Forests, and stacking ensembles are all options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec76d59",
   "metadata": {},
   "source": [
    "XGBoost Supports parallel processing,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4959b2a1",
   "metadata": {},
   "source": [
    "### 4. What is the advantage of evaluating out of the bag?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fd8f8f",
   "metadata": {},
   "source": [
    "- No leakage of data: Since the model is validated on the OOB Sample, which means data hasn’t been used while training the model in any way, so there isn’t any leakage of data and henceforth ensures a better predictive model.\n",
    "\n",
    "- Less Variance :  [More Variance ~ Overfitting due to more training score and less testing score]. Since OOB_Score ensures no leakage, so there is no over-fitting of the data and hence least variance.\n",
    "\n",
    "- Better Predictive Model: OOB_Score helps in the least variance and hence it makes a much better predictive model than a model using other validation techniques.\n",
    "\n",
    "- Less Computation: It requires less computation as it allows one to test the data as it is being trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d2544b",
   "metadata": {},
   "source": [
    "### 5. What distinguishes Extra-Trees from ordinary Random Forests? What good would this extra randomness do? Is it true that Extra-Tree Random Forests are slower or faster than normal Random Forests?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed9052",
   "metadata": {},
   "source": [
    "So, the main two differences are the following:\n",
    "\n",
    "- Random forest uses bootstrap replicas, that is to say, it subsamples the input data with replacement, whereas Extra Trees use the whole original sample. In the Extra Trees sklearn implementation there is an optional parameter that allows users to bootstrap replicas, but by default, it uses the entire input sample. This may increase variance because bootstrapping makes it more diversified.\n",
    "\n",
    "\n",
    "- Another difference is the selection of cut points in order to split nodes. Random Forest chooses the optimum split while Extra Trees chooses it randomly. However, once the split points are selected, the two algorithms choose the best one between all the subset of features. Therefore, Extra Trees adds randomization but still has optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b016fa2",
   "metadata": {},
   "source": [
    "In terms of computational cost, and therefore execution time, the Extra Trees algorithm is faster. This algorithm saves time because the whole procedure is the same, but it randomly chooses the split point and does not calculate the optimal one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4c7857",
   "metadata": {},
   "source": [
    "### 6. Which hyperparameters and how do you tweak if your AdaBoost ensemble underfits the training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4efebf7",
   "metadata": {},
   "source": [
    "- increasing the number of estimators \n",
    "\n",
    "- reducing the regularization hyperparameters of the base estimator. \n",
    "\n",
    "- slightly increasing the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b06c46",
   "metadata": {},
   "source": [
    "### 7. Should you raise or decrease the learning rate if your Gradient Boosting ensemble overfits the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eec8dc1",
   "metadata": {},
   "source": [
    "If your Gradient Boosting ensemble overfits the training set, you should try decreasing the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4278987e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
