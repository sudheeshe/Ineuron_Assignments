{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efde69b4",
   "metadata": {},
   "source": [
    "### 1. What is the concept of supervised learning? What is the significance of the name?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93917e9",
   "metadata": {},
   "source": [
    "Supervised learning, also known as supervised machine learning, is a subcategory of machine learning and artificial intelligence. It is defined by its use of labeled datasets to train algorithms that to classify data or predict outcomes accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e4baa",
   "metadata": {},
   "source": [
    "It is called supervised learning because the process of an algorithm learning from the training dataset can be thought of as a teacher supervising the learning process. We know the correct answers, the algorithm iteratively makes predictions on the training data and is corrected by the teacher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d401377",
   "metadata": {},
   "source": [
    "### 2. In the hospital sector, offer an example of supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ef5096",
   "metadata": {},
   "source": [
    "Cancer detection and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0349cead",
   "metadata": {},
   "source": [
    "### 3. Give three supervised learning examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6200e533",
   "metadata": {},
   "source": [
    "Random Forest, Decision tree, Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f96c955",
   "metadata": {},
   "source": [
    "### 4. In supervised learning, what are classification and regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce6372f",
   "metadata": {},
   "source": [
    "Classification and Regression are types of Supervised ML. Classification is the task of predicting a discrete class label. Regression is the task of predicting a continuous quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b733d6f",
   "metadata": {},
   "source": [
    "### 5. Give some popular classification algorithms as examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6db531",
   "metadata": {},
   "source": [
    "- SVM Classifier, \n",
    "- Decision Tree Classifier, \n",
    "- Logistic Regression, \n",
    "- Random Forest Classifier, \n",
    "- XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b350b36",
   "metadata": {},
   "source": [
    "### 6. Briefly describe the SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab65414",
   "metadata": {},
   "source": [
    "The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane.\n",
    "\n",
    "SVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme cases are called as support vectors, and hence algorithm is termed as Support Vector Machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aea7ea",
   "metadata": {},
   "source": [
    "### 7. In SVM, what is the cost of misclassification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c8c7b8",
   "metadata": {},
   "source": [
    "### 8. In the SVM model, define Support Vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d38dc8",
   "metadata": {},
   "source": [
    "The data points or vectors that are the closest to the hyperplane and which affect the position of the hyperplane are termed as Support Vector. Since these vectors support the hyperplane, hence called a Support vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1536139f",
   "metadata": {},
   "source": [
    "### 9. In the SVM model, define the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5bc667",
   "metadata": {},
   "source": [
    "“Kernel” is used due to a set of mathematical functions used in Support Vector Machine providing the window to manipulate the data. So, Kernel Function generally transforms the training set of data so that a non-linear decision surface is able to transform to a linear equation in a higher number of dimension spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1af0e2",
   "metadata": {},
   "source": [
    "### 10. What are the factors that influence SVM's effectiveness?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f40418",
   "metadata": {},
   "source": [
    "A) Selection of Kernel\n",
    "\n",
    "B) Kernel Parameters\n",
    "\n",
    "C) Soft Margin Parameter C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d761760e",
   "metadata": {},
   "source": [
    "### 11. What are the benefits of using the SVM model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f4444",
   "metadata": {},
   "source": [
    "- SVM works relatively well when there is a clear margin of separation between classes.\n",
    "- SVM is more effective in high dimensional spaces.\n",
    "- SVM is effective in cases where the number of dimensions is greater than the number of samples.\n",
    "- SVM is relatively memory efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1c3515",
   "metadata": {},
   "source": [
    "### 12.  What are the drawbacks of using the SVM model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ccfbe",
   "metadata": {},
   "source": [
    "- SVM algorithm is not suitable for large data sets.\n",
    "- SVM does not perform very well when the data set has more noise i.e. target classes are overlapping.\n",
    "- In cases where the number of features for each data point exceeds the number of training data samples, the SVM will underperform.\n",
    "- As the support vector classifier works by putting data points, above and below the classifying hyperplane there is no probabilistic explanation for the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5521f90a",
   "metadata": {},
   "source": [
    "### 13. Notes should be written on\n",
    "\n",
    "1. The kNN algorithm has a validation flaw.\n",
    "\n",
    "2. In the kNN algorithm, the k value is chosen.\n",
    "\n",
    "3. A decision tree with inductive bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6756195",
   "metadata": {},
   "source": [
    "`The kNN algorithm has a validation flaw.`\n",
    "\n",
    "No Training Period: KNN is called Lazy Learner (Instance based learning). It does not learn anything in the training period. It does not derive any discriminative function from the training data. In other words, there is no training period for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ae52e",
   "metadata": {},
   "source": [
    "`How value of K is choosen`\n",
    "\n",
    "K is the value indicates the count of the nearest neighbors\n",
    "\n",
    "There are no pre-defined statistical methods to find the most favorable value of K.\n",
    "- Initialize a random K value and start computing.\n",
    "- Choosing a small value of K leads to unstable decision boundaries.\n",
    "- The substantial K value is better for classification as it leads to smoothening the decision boundaries.\n",
    "- Derive a plot between error rate and K denoting values in a defined range. Then choose the K value as having a minimum error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d525a9",
   "metadata": {},
   "source": [
    "`A decision tree with inductive bias`\n",
    "\n",
    "n the case of decision trees, the depth of the tress is the inductive bias. If the depth of the tree is too low, then there is too much generalisation in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6d0313",
   "metadata": {},
   "source": [
    "### 14. What are some of the benefits of the kNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a61dc",
   "metadata": {},
   "source": [
    "- Simple to implement and intuitive to understand\n",
    "- Can learn non-linear decision boundaries when used for classfication and regression. Can came up with a highly flexible decision boundary adjusting the value of K.\n",
    "- No Training Time for classification/regression : The KNN algorithm has no explicit training step and all the work happens during prediction\n",
    "- Constantly evolves with new data: Since there is no explicit training step, as we keep adding new data to the dataset, the prediction is adjusted without having to retrain a new model.\n",
    "- Single Hyperparameters: There is a single hyperparameter, the value of K. This makes hyper parameter tuning easy.\n",
    "- Choice of distance metric: There are many distance metrics to chose from. Some popular distance metrics used are Euclidean, Manhattan, Minkowski, hamming distance eand so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52231f",
   "metadata": {},
   "source": [
    "### 15. What are some of the kNN algorithm's drawbacks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e315a9b5",
   "metadata": {},
   "source": [
    "- High prediction complexity for large datasets: Not great for large datasets, since the entire training data is processed for every prediction. Time complexity for each prediction is O(MNlog(k)) where M is the dimension of the data, N is the size or the number of instances in the training data. Note that there are specialized ways of organizing data to address this issue and make KNN faster.\n",
    "- Higher prediction complexity with higher dimensions:  The prediction compleixty in supervised learning gets higher for higher dimensional data (see the dependence of time complexity from the previous point on the dimension d).\n",
    "- KNN Assumes equal importance to all features: Since KNN expects points to be close in ALL dimensions, it might not consider points that are really close in sevaral dimensions, though farther away in a few favourably. This can be adjusted by chosing an appropriate distance measure. Moreover, this means it is sensitive if different features have different ranges. This can be addressed by appropriate pre-processing to scale features.\n",
    "- Sensitive to outliers: A single mislabeled example can change the class boundaries. This could specially be a bigger problem for larger dimensions, if there is an outlier in one dimension, since the average separation tends to be higher for higher dimensions (curse of dimensionality), outliers can have a bigger impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a8b64",
   "metadata": {},
   "source": [
    "### 16. Explain the decision tree algorithm in a few words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caefda2",
   "metadata": {},
   "source": [
    "A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes.\n",
    "\n",
    "Decision Tree : Decision tree is the most powerful and popular tool for classification and prediction. A Decision tree is a flowchart-like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe07ba",
   "metadata": {},
   "source": [
    "### 17. What is the difference between a node and a leaf in a decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bca3be",
   "metadata": {},
   "source": [
    "`Nodes` in Decision trees are the decision making point, Nodes can be divided into other sub nodes based on the condition at written on the nodes\n",
    "\n",
    "The `leaf nodes` (green), also called terminal nodes, are nodes that don't split into more nodes. Leaf nodes are where classes are assigned by majority vote."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447c14da",
   "metadata": {},
   "source": [
    "### 18. What is a decision tree's entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c97d14",
   "metadata": {},
   "source": [
    "Entropy is an information theory metric that measures the impurity or uncertainty in a group of observations. It determines how a decision tree chooses to split data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c2a108",
   "metadata": {},
   "source": [
    "### 19. In a decision tree, define knowledge gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eb6254",
   "metadata": {},
   "source": [
    "The information gained in the decision tree can be defined as the amount of information improved in the nodes before splitting them for making further decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6928bbe",
   "metadata": {},
   "source": [
    "### 20. Choose three advantages of the decision tree approach and write them down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e0d2b",
   "metadata": {},
   "source": [
    "- Simple to understand and to interpret. Trees can be visualized.\n",
    "\n",
    "- Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n",
    "\n",
    "- The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "\n",
    "- Able to handle both numerical and categorical data. However scikit-learn implementation does not support categorical variables for now. Other techniques are usually specialized in analyzing datasets that have only one type of variable. See algorithms for more information.\n",
    "\n",
    "- Able to handle multi-output problems.\n",
    "\n",
    "- Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n",
    "\n",
    "- Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "\n",
    "- Performs well even if its assumptions are somewhat violated by the true model from which the data were generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9499fa90",
   "metadata": {},
   "source": [
    "### 21. Make a list of three flaws in the decision tree process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64275bc",
   "metadata": {},
   "source": [
    "- Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "\n",
    "- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "\n",
    "- Predictions of decision trees are neither smooth nor continuous, but piecewise constant approximations as seen in the above figure. Therefore, they are not good at extrapolation.\n",
    "\n",
    "- The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n",
    "\n",
    "- There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "\n",
    "- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a235713",
   "metadata": {},
   "source": [
    "### 22. Briefly describe the random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b19b9",
   "metadata": {},
   "source": [
    "Random forest is a Supervised Machine Learning Algorithm that is used widely in Classification and Regression problems. It builds decision trees on different samples and takes their majority vote for classification and average in case of regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660e4166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
