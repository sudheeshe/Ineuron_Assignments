{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c21dc7dc",
   "metadata": {},
   "source": [
    "### 1. What are the key reasons for reducing the dimensionality of a dataset? What are the major disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6470db60",
   "metadata": {},
   "source": [
    "`Advantages of Dimensionality Reduction`\n",
    "\n",
    "- It helps in data compression, and hence reduced storage space.\n",
    "- It reduces computation time.\n",
    "- It also helps remove redundant features, if any.\n",
    "\n",
    "\n",
    "`Disadvantages of Dimensionality Reduction`\n",
    "\n",
    "- It may lead to some amount of data loss.\n",
    "- PCA tends to find linear correlations between variables, which is sometimes undesirable.\n",
    "- PCA fails in cases where mean and covariance are not enough to define datasets.\n",
    "- We may not know how many principal components to keep- in practice, some thumb rules are applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5399ae",
   "metadata": {},
   "source": [
    "### 2. What is the dimensionality curse?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f7402",
   "metadata": {},
   "source": [
    "Curse of Dimensionality describes the explosive nature of increasing data dimensions and its resulting exponential increase in computational efforts required for its processing and/or analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7bd24",
   "metadata": {},
   "source": [
    "### 3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how can you go about doing it? If not, what is the reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c454853e",
   "metadata": {},
   "source": [
    "No, dimensionality reduction is not reversible in general. It loses information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49700c5",
   "metadata": {},
   "source": [
    "### 4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0634615e",
   "metadata": {},
   "source": [
    "PCA can be used to significantly reduce the dimensionality of most datasets, even if they are highly nonlinear because it can at least get rid of useless dimensions. However, if there are no useless dimensions, reducing dimensionality with PCA will lose too much information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d8a49",
   "metadata": {},
   "source": [
    "### 5. Assume you're running PCA on a 1,000-dimensional dataset with a 95 percent explained variance ratio. What is the number of dimensions that the resulting dataset would have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e398daa",
   "metadata": {},
   "source": [
    "In Scikit-learn, PCA is applied using the PCA() class. It is in the decomposition submodule in Scikit-learn. The most important hyperparameter in that class is n_components. It can take one of the following types of values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469fabef",
   "metadata": {},
   "source": [
    "If 0 < n_components < 1, PCA will select the number of components such that the amount of variance that needs to be explained. \n",
    "\n",
    "For example, if n_components=0.95, the algorithm will select the number of components while preserving 95% of the variability in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d55803a",
   "metadata": {},
   "source": [
    "### 6. Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece31366",
   "metadata": {},
   "source": [
    "`Principal component analysis (PCA)` using randomized SVD. Linear dimensionality reduction using approximated Singular Value Decomposition of the data and keeping only the most significant singular vectors to project the data to a lower dimensional space.\n",
    "\n",
    "\n",
    "`Incremental principal component analysis (IPCA)` is typically used as a replacement for principal component analysis (PCA) when the dataset to be decomposed is too large to fit in memory.\n",
    "\n",
    "`Kernel principal component analysis` (kernel PCA) is an extension of principal component analysis (PCA) using techniques of kernel methods. Using a kernel, the originally linear operations of PCA are performed in a reproducing kernel Hilbert space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8743d0aa",
   "metadata": {},
   "source": [
    "### 7. How do you assess a dimensionality reduction algorithm's success on your dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c82868e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12001cc7",
   "metadata": {},
   "source": [
    "### 8. Is it logical to use two different dimensionality reduction algorithms in a chain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a45686",
   "metadata": {},
   "source": [
    "It can make sense to combine two DR methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f074d74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
