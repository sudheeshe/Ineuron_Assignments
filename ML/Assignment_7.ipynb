{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9206b132",
   "metadata": {},
   "source": [
    "### 1. What is the definition of a target function? In the sense of a real-life example, express the target function. How is a target function's fitness assessed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f42a17",
   "metadata": {},
   "source": [
    "A `target function`, in machine learning, is a method for solving a problem that an AI algorithm parses its training data to find. Once an algorithm finds its target function, that function can be used to predict results (predictive analysis). The function can then be used to find output data related to inputs\n",
    "\n",
    "We can use performance matrices based on the problem statement the taret function trying to solve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4589c9c4",
   "metadata": {},
   "source": [
    "### 2. What are predictive models, and how do they work? What are descriptive types, and how do you use them? Examples of both types of models should be provided. Distinguish between these two forms of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3bd436",
   "metadata": {},
   "source": [
    "`Predictive modeling` is a commonly used statistical technique to predict future behavior. Predictive modeling solutions are a form of data-mining technology that works by analyzing historical and current data and generating a model to help predict future outcomes.\n",
    "\n",
    "It works by analyzing current and historical data and projecting what it learns on a model generated to forecast likely outcomes.\n",
    "\n",
    "A `descriptive model` describes a system or other entity and its relationship to its environment. It is generally used to help specify and/or understand what the system is, what it does, and how it does it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b4e159",
   "metadata": {},
   "source": [
    "A `Descriptive model` will exploit the past data that are stored in databases and provide you with the accurate report. In a `Predictive model`, it identifies patterns found in past and transactional data to find risks and future outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe55782",
   "metadata": {},
   "source": [
    "Example of Descriptive model is sequence discovery\n",
    "\n",
    "Example of Classification or Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9f69cb",
   "metadata": {},
   "source": [
    "### 3. Describe the method of assessing a classification model's efficiency in detail. Describe the various measurement parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247c8428",
   "metadata": {},
   "source": [
    "`1) Accuracy`\n",
    "\n",
    "Accuracy simply measures how often the classifier correctly predicts. We can define accuracy as the ratio of the number of correct predictions and the total number of predictions.\n",
    "\n",
    "`2) Confusion Matrix`\n",
    "\n",
    "Confusion Matrix is a performance measurement for the machine learning classification problems where the output can be two or more classes. It is a table with combinations of predicted and actual values.\n",
    "\n",
    "`3) Precision`\n",
    "\n",
    "Precision explains how many of the correctly predicted cases actually turned out to be positive. Precision is useful in the cases where False Positive is a higher concern than False Negatives.\n",
    "\n",
    "`4) Recall (Sensitivity)`\n",
    "\n",
    "Recall explains how many of the actual positive cases we were able to predict correctly with our model. It is a useful metric in cases where False Negative is of higher concern than False Positive.\n",
    "\n",
    "`5) F1 Score` \n",
    "\n",
    "It gives a combined idea about Precision and Recall metrics. It is maximum when Precision is equal to Recall.\n",
    "\n",
    "`6) AUC-ROC` \n",
    "\n",
    "The Receiver Operator Characteristic (ROC) is a probability curve that plots the TPR(True Positive Rate) against the FPR(False Positive Rate) at various threshold values and separates the ‘signal’ from the ‘noise’.\n",
    "\n",
    "The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes. From the graph, we simply say the area of the curve ABDE and the X and Y-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef5c09",
   "metadata": {},
   "source": [
    "### 4. \n",
    "      i. In the sense of machine learning models, what is underfitting? What is the most common reason for underfitting?\n",
    "     ii. What does it mean to overfit? When is it going to happen?\n",
    "    iii. In the sense of model fitting, explain the bias-variance trade-off.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aafd5b0",
   "metadata": {},
   "source": [
    "`Underfitting:`\n",
    "\n",
    "Underfitting the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples (often called X) and the target values (often called Y)\n",
    "\n",
    "The training data is not modelled and even no generalization of new data. Which gives poor performance in training and testinf data\n",
    "\n",
    "Reasons are:\n",
    "\n",
    "- Data used for training is not cleaned and contains noise (garbage values) in it.\n",
    "- The model has a high bias.\n",
    "- The size of the training dataset used is not enough.\n",
    "- The model is too simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2edfa7",
   "metadata": {},
   "source": [
    "`Overfitting:`\n",
    "\n",
    "overfitting your training data when you see that the model performs well on the training data but does not perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples.\n",
    "\n",
    "Reasons are:\n",
    "\n",
    "- Too complex model\n",
    "\n",
    "- Data has noise i.e. like there are outliers and errors in data\n",
    "\n",
    "- Size of data used for training may not be enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3df577",
   "metadata": {},
   "source": [
    "`Bias Variance Tradeoff: `\n",
    "    \n",
    "`Bias` is the difference between the average prediction of our model and the correct value which we are trying to predict. Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to high error on training and test data.\n",
    "\n",
    "\n",
    "`Variance` is the variability of model prediction for a given data point or a value which tells us spread of our data. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f08c69",
   "metadata": {},
   "source": [
    "`Why is Bias Variance Tradeoff?`\n",
    "\n",
    "If our model is too simple and has very few parameters then it may have high bias and low variance. On the other hand if our model has large number of parameters then it’s going to have high variance and low bias. So we need to find the right/good balance without overfitting and underfitting the data.\n",
    "\n",
    "This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a5993",
   "metadata": {},
   "source": [
    "### 5. Is it possible to boost the efficiency of a learning model? If so, please clarify how."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a59bba1",
   "metadata": {},
   "source": [
    "`1: Add more data samples :`\n",
    "        \n",
    "Doing so will add more details to your data and finetune your model resulting in a more accurate performance. Rember after all, the more information you give your model, the more it will learn and the more cases it will be able to identify correctly.\n",
    "\n",
    "`2: Finetune your hyperparameter`\n",
    "\n",
    "`3: Train your model using cross-validation`\n",
    "\n",
    "`4: Experiment with a different algorithm.`\n",
    "\n",
    "`5: Treat missing and Outlier values`\n",
    "\n",
    "`6: Feature Engineering`\n",
    "\n",
    "`7. Feature Selection`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e8e54a",
   "metadata": {},
   "source": [
    "### 6. How would you rate an unsupervised learning model's success? What are the most common success indicators for an unsupervised learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6276a72",
   "metadata": {},
   "source": [
    "Twin sample validation can be used to validate results of unsupervised learning. It should be used in combination with internal validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bffbd8",
   "metadata": {},
   "source": [
    "`Adjusted Rand Index`\n",
    "\n",
    "The Adjusted Rand Index is a variation on the classic Rand Index, and attempts to express what proportion of the cluster assignments are ‘correct’. It computes a similarity measure between two different clusterings by considering all pairs of samples, and counting pairs that are assigned in the same or different clusters predicted, against the true clusterings, adjusting for random chance.\n",
    "\n",
    "\n",
    "`Fowlkes Mallows Score`\n",
    "\n",
    "The Fowlkes Mallow Score is similar, in as much that it tells you the degree to which cluster assignments are ‘correct’. In particular, it calculates the geometric mean between precision and recall. It’s bounded between 0 and 1, with higher values being better.\n",
    "\n",
    "`Silhouette Score`\n",
    "\n",
    "The Silhouette Score attempts to describe how similar a datapoint is to other datapoints in its cluster, relative to datapoints not in its cluster (this is aggregated over all datapoints to get the score for an overall clustering). In other words, it thinks about how ‘distinct’ the clusters are in space — indeed one could use any measure of ‘distance’ to calculate the score.\n",
    "\n",
    "`Calinski Harabaz Index`\n",
    "\n",
    "The Calinski Harabaz Index is the ratio of the variance of a datapoint compared to points in other clusters, against the variance compared to points within its cluster. Since we want this first part to be high, and the second part to be low, a high CH index is desirable. Unlike other metrics we have seen, this score is not bounded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7f0a02",
   "metadata": {},
   "source": [
    "### 7. Is it possible to use a classification model for numerical data or a regression model for categorical data with a classification model? Explain your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81948fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c23821ab",
   "metadata": {},
   "source": [
    "### 8. Describe the predictive modeling method for numerical values. What distinguishes it from categorical predictive modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7397a1",
   "metadata": {},
   "source": [
    "In short, predictive modeling is a statistical technique using machine learning and data mining to predict and forecast likely future outcomes with the aid of historical and existing data. It works by analyzing current and historical data and projecting what it learns on a model generated to forecast likely outcomes.\n",
    "\n",
    "\n",
    "Numerical data refers to the data that is in the form of numbers, and not in any language or descriptive form. Predictive modelling of numerical values is comparitively easy and very less preprocessing is required\n",
    "\n",
    "Categorical predictive modelling are time consuming with multiple feature engineering neededs. Since ML models only accepts numerical data for learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327288bd",
   "metadata": {},
   "source": [
    "### 9. The following data were collected when using a classification model to predict the malignancy of a group of patients' tumors:\n",
    "\n",
    "         i. Accurate estimates – 15 cancerous, 75 benign\n",
    "         ii. Wrong predictions – 3 cancerous, 7 benign\n",
    "      \n",
    "      Determine the model's error rate, Kappa value, sensitivity, precision, and F-measure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d398100",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 15\n",
    "TN = 75\n",
    "\n",
    "FP = 3\n",
    "FN = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f270eb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate is 0.1\n"
     ]
    }
   ],
   "source": [
    "Error_rate = (FP + FN) / (TP + TN + FN + FP)\n",
    "print(f\"Error Rate is {Error_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2cbe49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa_value is 0.688279301745636\n"
     ]
    }
   ],
   "source": [
    "total_accuracy = (TP + TN) / (TN+TP+FN+FP)\n",
    "\n",
    "random_accuracy = (0.22 * .18 + 0.78 * 0.82)\n",
    "\n",
    "Kappa_value =  (total_accuracy - random_accuracy) / (1- random_accuracy)\n",
    "\n",
    "print(f\"Kappa_value is {Kappa_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3672a580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa_value is 0.688279301745636\n"
     ]
    }
   ],
   "source": [
    "Precision = TP / (TP + FP)\n",
    "\n",
    "print(f\"Kappa_value is {Kappa_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b54cd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall is 0.6818181818181818\n"
     ]
    }
   ],
   "source": [
    "Recall = TP / (TP + FN)\n",
    "\n",
    "print(f\"Recall is {Recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa278c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_Score is 0.7499999999999999\n"
     ]
    }
   ],
   "source": [
    "F1_Score = 2* ((Precision * Recall) / (Precision + Recall))\n",
    "\n",
    "print(f\"F1_Score is {F1_Score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f495b6d",
   "metadata": {},
   "source": [
    "### 10. Make quick notes on:\n",
    "         1. The process of holding out\n",
    "         2. Cross-validation by tenfold\n",
    "         3. Adjusting the parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2999d0",
   "metadata": {},
   "source": [
    "`Holdout Method` is the simplest sort of method to evaluate a classifier. In the holdout method, data set is partitioned, such that – maximum data belongs to training set and remaining data belongs to test set.\n",
    "\n",
    "In this technique is an exhaustive cross-validation method, that randomly splits the dataset into train and test data depending on data analysis. In the case of holdout cross-validation, the dataset is randomly split into training and validation data. Generally, the split of training data is more than test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d55817f",
   "metadata": {},
   "source": [
    "`10-fold cross validation` would perform the fitting procedure a total of ten times, with each fit being performed on a training set consisting of 90% of the total training set selected at random, with the remaining 10% used as a hold out set for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e65e3",
   "metadata": {},
   "source": [
    "`Hyperparameter optimization or tuning` is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1466dae3",
   "metadata": {},
   "source": [
    "### 11. Define the following terms: \n",
    "\n",
    "         1. Purity vs. Silhouette width\n",
    "         2. Boosting vs. Bagging\n",
    "         3. The eager learner vs. the lazy learner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b603502",
   "metadata": {},
   "source": [
    "`Puriy:`\n",
    "    \n",
    "Purity is quite simple to calculate. We assign a label to each cluster based on the most frequent class in it. Then the purity becomes the number of correctly matched class and cluster labels divided by the number of total data points. Each cluster is assigned with the most frequent class label.\n",
    "\n",
    "`Silhouette score:`\n",
    "\n",
    "Silhouette score is used to evaluate the quality of clusters created using clustering algorithms such as K-Means in terms of how well samples are clustered with other samples that are similar to each other. The Silhouette score is calculated for each sample of different clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95082a6",
   "metadata": {},
   "source": [
    "`Bagging:` \n",
    "\n",
    "It is a homogeneous weak learners’ model that learns from each other independently in parallel and combines them for determining the model average.\n",
    "\n",
    "`Boosting:` \n",
    "\n",
    "It is also a homogeneous weak learners’ model but works differently from Bagging. In this model, learners learn sequentially and adaptively to improve model predictions of a learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc631d47",
   "metadata": {},
   "source": [
    "`Lazy learner vs Eager leaner`\n",
    "\n",
    "A lazy learner delays abstracting from the data until it is asked to make a prediction while an eager learner abstracts away from the data during training and uses this abstraction to make predictions rather than directly compare queries with instances in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2bdd52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
