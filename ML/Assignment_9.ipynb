{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee8343b6",
   "metadata": {},
   "source": [
    "### 1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12220735",
   "metadata": {},
   "source": [
    "`Feature engineering` \n",
    "\n",
    "Feature engineering is a machine learning technique that leverages data to create new variables that aren’t in the training set. It can produce new features for both supervised and unsupervised learning, with the goal of simplifying and speeding up data transformations while also enhancing model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dfb000",
   "metadata": {},
   "source": [
    "Feature engineering involves several processes. Feature selection, construction, transformation, and extraction are some key aspects of feature engineering. Let’s understand what each process involves:\n",
    "\n",
    "- Feature selection\n",
    "\n",
    "involves choosing a set of features from a large collection. Selecting the important features and reducing the size of the feature set makes computation in machine learning and data analytic algorithms more feasible. Feature selection also improves the quality of the output obtained from algorithms. \n",
    "\n",
    "\n",
    "- Feature transformation\n",
    "\n",
    "involves creating features using existing data by the use of mathematical operations. For example, to ascertain the body type of a person a feature called BMI (Body Mass Index) is needed. If the dataset captures the person’s weight and height, BMI can be derived using a mathematical formula.\n",
    "\n",
    "\n",
    "- Feature construction \n",
    "\n",
    "is the process of developing new features apart from the ones generated in feature transformation, that are appropriate variables of the process under study.\n",
    "\n",
    "\n",
    "- Feature extraction\n",
    "\n",
    "is a process of reducing the dimensionality of a dataset. Feature extraction involves combining the existing features into new ones thereby reducing the number of features in the dataset. This reduces the amount of data into manageable sizes for algorithms to process, without distorting the original relationships or relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55202138",
   "metadata": {},
   "source": [
    "### 2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044f5d5",
   "metadata": {},
   "source": [
    "Feature Selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in data. It is the process of automatically choosing relevant features for your machine learning model based on the type of problem you are trying to solve.\n",
    "\n",
    "There are three types of feature selection: \n",
    "\n",
    "- Wrapper methods (forward, backward, and stepwise selection), \n",
    "- Filter methods (ANOVA, Pearson correlation, variance thresholding), \n",
    "- Embedded methods (Lasso, Ridge, Decision Tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe5c472",
   "metadata": {},
   "source": [
    "### 3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f12963b",
   "metadata": {},
   "source": [
    "`Filter Method:`\n",
    "    \n",
    "features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. Like Pearson Correlation, Chisquare, Anova, LDA etc..\n",
    "\n",
    "`Wrapper Method:`\n",
    "\n",
    "In wrapper methods, we try to use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset. The problem is essentially reduced to a search problem. These methods are usually computationally very expensive.\n",
    "\n",
    "Some common examples of wrapper methods are forward feature selection, backward feature elimination, recursive feature elimination, etc.\n",
    "\n",
    "- Forward Selection: \n",
    "\n",
    "Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n",
    "\n",
    "- Backward Elimination: \n",
    "\n",
    "In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n",
    "\n",
    "- Recursive Feature elimination: \n",
    "\n",
    "It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa61eaf",
   "metadata": {},
   "source": [
    "Difference between Filter and Wrapper methods\n",
    "\n",
    "The main differences between the filter and wrapper methods for feature selection are:\n",
    "\n",
    "- Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "\n",
    "- Filter methods are much faster compared to wrapper methods as they do not involve training the models. On the other hand, wrapper methods are computationally very expensive as well.\n",
    "\n",
    "- Filter methods use statistical methods for evaluation of a subset of features while wrapper methods use cross validation.\n",
    "\n",
    "- Filter methods might fail to find the best subset of features in many occasions but wrapper methods can always provide the best subset of features.\n",
    "\n",
    "- Using the subset of features from the wrapper methods make the model more prone to overfitting as compared to using subset of features from the filter methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623217b",
   "metadata": {},
   "source": [
    "4.\n",
    "\n",
    "    i. Describe the overall feature selection process.\n",
    "\n",
    "    ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2c8d01",
   "metadata": {},
   "source": [
    "Feature Selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in data. It is the process of automatically choosing relevant features for your machine learning model based on the type of problem you are trying to solve.\n",
    "\n",
    "There are three types of feature selection:\n",
    "\n",
    "- Wrapper methods (forward, backward, and stepwise selection),\n",
    "- Filter methods (ANOVA, Pearson correlation, variance thresholding),\n",
    "- Embedded methods (Lasso, Ridge, Decision Tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb290da",
   "metadata": {},
   "source": [
    "`Feature extraction`\n",
    "\n",
    "It is a process of reducing the dimensionality of a dataset. Feature extraction involves combining the existing features into new ones thereby reducing the number of features in the dataset. This reduces the amount of data into manageable sizes for algorithms to process, without distorting the original relationships or relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d143f801",
   "metadata": {},
   "source": [
    "`principle of feature extraction`\n",
    "\n",
    "The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a98e2a",
   "metadata": {},
   "source": [
    "- Principal Component Analysis (PCA)\n",
    "- Linear discriminant analysis (LDA)\n",
    "- Generalized discriminant analysis (GDA)\n",
    "- Low Variance Filter.\n",
    "- High Correlation Filter.\n",
    "- Backward Feature Elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808eea55",
   "metadata": {},
   "source": [
    "### 5. Describe the feature engineering process in the sense of a text categorization issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6858ff80",
   "metadata": {},
   "source": [
    "1) Removing stop words\n",
    "\n",
    "2) Tokennizing the words\n",
    "\n",
    "3) Converting words to Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3b5c46",
   "metadata": {},
   "source": [
    "### 6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a154b355",
   "metadata": {},
   "source": [
    "Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction. It is often used to measure document similarity in text analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9738e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "dataSetI = [2, 3, 2, 0, 2, 3, 3, 0, 1]\n",
    "dataSetII = [2, 1, 0, 0, 3, 2, 1, 3, 1]\n",
    "result = 1 - spatial.distance.cosine(dataSetI, dataSetII)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89b125",
   "metadata": {},
   "source": [
    "7.\n",
    "\n",
    "    i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "\n",
    "    ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b3bfb7",
   "metadata": {},
   "source": [
    "`Hamming Distance`\n",
    "\n",
    "The Hamming distance between two equal-length strings of symbols is the number of positions at which the corresponding symbols are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e1190a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def hammingDist(str1, str2):\n",
    "    i = 0\n",
    "    count = 0\n",
    " \n",
    "    while(i < len(str1)):\n",
    "        if(str1[i] != str2[i]):\n",
    "            count += 1\n",
    "        i += 1\n",
    "    return count\n",
    " \n",
    "# Driver code \n",
    "str1 = \"10001011\"\n",
    "str2 = \"11001111\"\n",
    " \n",
    "# function call\n",
    "print(hammingDist(str1, str2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20bb76f",
   "metadata": {},
   "source": [
    "`Jaccard similarity index`\n",
    "\n",
    "sometimes called the Jaccard similarity coefficient compares members for two sets to see which members are shared and which are distinct. It's a measure of similarity for the two sets of data, with a range from 0% to 100%. The higher the percentage, the more similar the two populations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d603a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(set(list1)) + len(set(list2))) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69144964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = [1, 1, 0, 0, 1, 0, 1, 1]\n",
    "list2 = [1, 1, 0, 0, 0, 1, 1, 1]\n",
    "\n",
    "\n",
    "jaccard_similarity(list1, list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a98be2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = [1, 0, 0, 1, 1, 0, 0, 1]\n",
    "list2 = [1, 1, 0, 0, 0, 1, 1, 1]\n",
    "\n",
    "jaccard_similarity(list1, list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5633781c",
   "metadata": {},
   "source": [
    "The `simple matching coefficient (SMC) or Rand similarity coefficient` is a statistic used for comparing the similarity and diversity of sample sets. Given two objects, A and B, each with n binary attributes, SMC is defined as: where: is the total number of attributes where A and B both have a value of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7755c7e7",
   "metadata": {},
   "source": [
    "### 8. State what is meant by  \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc90f65",
   "metadata": {},
   "source": [
    "High dimensional data refers to a dataset in which the number of features p is larger than the number of observations N, often written as p >> N.\n",
    "\n",
    "Eg:\n",
    "\n",
    "1) Healthcare Data\n",
    "\n",
    "High dimensional data is common in healthcare datasets where the number of features for a given individual can be massive (i.e. blood pressure, resting heart rate, immune system status, surgery history, height, weight, existing conditions, etc.).\n",
    "\n",
    "2) Financial Data\n",
    "\n",
    "High dimensional data is also common in financial datasets where the number of features for a given stock can be quite large (i.e. PE Ratio, Market Cap, Trading Volume, Dividend Rate, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279c6b55",
   "metadata": {},
   "source": [
    "`Challenges with High Dimensional Data :`\n",
    "    \n",
    "    Dimensionally cursed phenomena occur in domains such as numerical analysis, sampling, combinatorics, machine learning, data mining and databases. The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e797e33",
   "metadata": {},
   "source": [
    "- If we have more features than observations than we run the risk of massively overfitting our model — this would generally result in terrible out of sample performance.\n",
    "\n",
    "- When we have too many features, observations become harder to cluster — believe it or not, too many dimensions causes every observation in your dataset to appear equidistant from all the others. And because clustering uses a distance measure such as Euclidean distance to quantify the similarity between observations, this is a big problem. If the distances are all approximately equal, then all the observations appear equally alike (as well as equally different), and no meaningful clusters can be formed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4a0699",
   "metadata": {},
   "source": [
    "Soltions are:\n",
    "    \n",
    "    1) Feature Selection\n",
    "    2) Dimensionality reduction\n",
    "    3) Use a regularization method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1841d156",
   "metadata": {},
   "source": [
    "### 9. Make a few quick notes on:\n",
    "\n",
    "1. PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "2. Use of vectors\n",
    "\n",
    "3. Embedded technique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79057ced",
   "metadata": {},
   "source": [
    "`Principal Component Analysis:`\n",
    "    \n",
    "Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
    "\n",
    "\n",
    "`Vectors`\n",
    "\n",
    "Vectors are used to represent numeric characteristics, called features, of an object in a mathematical and easily analyzable way. Vectors are essential for many different areas of machine learning and pattern processing.\n",
    "\n",
    "Vectors are used on NLP for embeddings\n",
    "\n",
    "`Embedded technique`\n",
    "\n",
    "An embedding is a low-dimensional translation of a high-dimensional vector. Embedding is the process of converting high-dimensional data to low-dimensional data in the form of a vector in such a way that the two are semantically similar. In the world of Natural Language Processing. They allow us to capture relationships in language that are very difficult to capture otherwise. However, embedding layers can be used to embed many more things than just words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78878dda",
   "metadata": {},
   "source": [
    "### 10. Make a comparison between:\n",
    "\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "\n",
    "3. SMC vs. Jaccard coefficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f02e411",
   "metadata": {},
   "source": [
    "`Forward Selection:` Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n",
    "\n",
    "`Backward elimination:` is a feature selection technique while building a machine learning model. It is used to remove those features that do not have a significant effect on the dependent variable or prediction of output. Backward selection starts with a (usually complete) set of variables and then excludes variables from that set, again, until some stopping criterion is met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8aade",
   "metadata": {},
   "source": [
    "`filter vs. wrapper`\n",
    "\n",
    "The main differences between the filter and wrapper methods for feature selection are: Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab3eabc",
   "metadata": {},
   "source": [
    "`SMC vs. Jaccard coefficient`\n",
    "\n",
    "The SMC is very similar to the more popular Jaccard index. the SMC counts both mutual presences (when an attribute is present in both sets) and mutual absence (when an attribute is absent in both sets) as matches and compares it to the total number of attributes in the universe, whereas the Jaccard index only counts mutual presence as matches and compares it to the number of attributes that have been chosen by at least one of the two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7279cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
