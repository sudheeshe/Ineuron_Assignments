{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262ba819",
   "metadata": {},
   "source": [
    "### 1.\tWhat does a SavedModel contain? How do you inspect its content?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bc5337",
   "metadata": {},
   "source": [
    "SavedModel refers to Serialization process of converting a data object (e.g. Tensorflow models) into a format that allows us to store or transmit the data and then recreate the object when needed using the reverse process of deserialization.\n",
    "\n",
    "We can use the SavedModel Command Line Interface (CLI) to inspect and execute a SavedModel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5e4ae",
   "metadata": {},
   "source": [
    "### 2.\tWhen should you use TF Serving? What are its main features? What are some tools you can use to deploy it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f49fff",
   "metadata": {},
   "source": [
    "TensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments. TensorFlow Serving makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs. TensorFlow Serving provides out-of-the-box integration with TensorFlow models, but can be easily extended to serve other types of models and data.\n",
    "\n",
    "TensorFlow Serving allows us to select which version of a model, or \"servable\" we want to use when we make inference requests. Each version will be exported to a different sub-directory under the given path.\n",
    "\n",
    "Tools are:\n",
    "Docker, Kubernetes, Azure, GCP or AWS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bdc284",
   "metadata": {},
   "source": [
    "### 3.\tHow do you deploy a model across multiple TF Serving instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e765c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "174e9537",
   "metadata": {},
   "source": [
    "### 4.\tWhen should you use the gRPC API rather than the REST API to query a model served by TF Serving?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7649978",
   "metadata": {},
   "source": [
    "gRPC is a modern, open source remote procedure call (RPC) framework that can run anywhere. It enables client and server applications to communicate transparently, and makes it easier to build connected systems.\n",
    "\n",
    "`What are the benefits of using gRPC?`\n",
    "\n",
    "- gRPC uses binary payloads, which are efficient to create and parse and hence light-weight.\n",
    "\n",
    "- Bi-directional streaming is possible in gRPC, which is not the case with REST API\n",
    "\n",
    "- gRPC API is built on top of HTTP/2 supporting the traditional request and response steaming as well as bi-directional streaming\n",
    "\n",
    "- 10 times faster message transmission compared to REST API as gRPC uses serialized Protocol Buffers and HTTP/2\n",
    "\n",
    "- Loose coupling between client and server makes it easy to make changes\n",
    "\n",
    "- gRPC allows integration of API’s programmed in different languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f27e768",
   "metadata": {},
   "source": [
    "`What’s the difference between gRPC and REST API?`\n",
    "\n",
    "- Payload Format: REST uses JSON for exchanging messages between client and server, whereas gRPC uses Protocol Buffers. Protocol Buffers are compressed better than JSON, thus making gRPC transmit data over networks more efficiently.\n",
    "\n",
    "- Transfer Protocols: REST heavily uses HTTP 1.1 protocol, which is textual, whereas gRPC is built on the new HTTP/2 binary protocol that compresses the header with efficient parsing and is much safer.\n",
    "\n",
    "- Streaming vs. Request-Response: REST supports the Request-Response model available in HTTP1.1. gRPC uses bi-directional streaming capabilities available in HTTP/2, where the client and server send a sequence of messages to each other using a read-write stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1489e2f0",
   "metadata": {},
   "source": [
    "### 5.\tWhat are the different ways TFLite reduces a model’s size to make it run on a mobile or embedded device?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25b8388",
   "metadata": {},
   "source": [
    "We can reduce the size of a Tensorflow Model using the below mentioned methods:\n",
    "\n",
    "- `Freezing:` Convert the variables stored in a checkpoint file of the SavedModel into constants stored directly in the model graph. This reduces the overall size of the model.\n",
    "\n",
    "- `Pruning:` Strip unused nodes in the prediction path and the outputs of the graph, merging duplicate nodes, as well as cleaning other node ops like summary, identity, etc.\n",
    "\n",
    "- `Constant folding:` Look for any sub-graphs within the model that always evaluate to constant expressions, and replace them with those constants. Folding batch norms: Fold the multiplications introduced in batch normalization into the weight multiplications of the previous layer.\n",
    "\n",
    "- `Quantization:` Convert weights from floating point to lower precision, such as 16 or 8 bits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f93cbc",
   "metadata": {},
   "source": [
    "### 6.\tWhat is quantization-aware training, and why would you need it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9316e872",
   "metadata": {},
   "source": [
    "Quantization-aware training helps you train DNNs for lower precision INT8 deployment, without compromising on accuracy. This is achieved by modeling quantization errors during training which helps in maintaining accuracy as compared to FP16 or FP32.\n",
    "\n",
    "QAT works by emulating the quantization losses that happen after quantizing the model in the training process. This means that the model will be aware of the losses that happen after quantization and it will learn to overcome them.\n",
    "\n",
    "Training with QAT helps to increase the accuracy of the network.There are a few potential reasons for it:\n",
    "\n",
    "- Finetuning: Since we convert a previously trained model, we are in-effect, continuing the training of previously trained model. This can be a reason for the increase in accuracy.\n",
    "\n",
    "- Regularization: Each operation in the model is now converted to its equivalent fake quantization operation. This could act like a regularization method and help improve the accuracy of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ea431a",
   "metadata": {},
   "source": [
    "### 7.\tWhat are model parallelism and data parallelism? Why is the latter generally recommended?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd79e06d",
   "metadata": {},
   "source": [
    "`Model parallelism` is a distributed training method in which the deep learning model is partitioned across multiple devices, within or across instances.\n",
    "\n",
    "`Data parallelism` refers to scenarios in which the same operation is performed concurrently (that is, in parallel) on elements in a source collection or array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1dbc61",
   "metadata": {},
   "source": [
    "### 8.\tWhen training a model across multiple servers, what distribution strategies can you use? How do you choose which one to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170db30",
   "metadata": {},
   "source": [
    "`Synchronous vs asynchronous training:` These are two common ways of distributing training with data parallelism. In sync training, all workers train over different slices of input data in sync, and aggregating gradients at each step. In async training, all workers are independently training over the input data and updating variables asynchronously. Typically sync training is supported via all-reduce and async through parameter server architecture.\n",
    "\n",
    "TensorFlow has \n",
    "\n",
    "- MirroredStrategy, \n",
    "- TPUStrategy, \n",
    "- MultiWorkerMirroredStrategy, \n",
    "- ParameterServerStrategy, \n",
    "- CentralStorageStrategy, \n",
    "- Default Strategy\n",
    "- OneDeviceStrategy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d7b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
