{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9877f31",
   "metadata": {},
   "source": [
    "### 1.\tWhy would you want to use the Data API?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af5ee91",
   "metadata": {},
   "source": [
    "APIs are needed to bring applications together in order to perform a designed function built around sharing data and executing pre-defined processes. They work as the middle man, allowing developers to build new programmatic interactions between the various applications people and businesses use on a daily basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7669a05",
   "metadata": {},
   "source": [
    "### 2.\tWhat are the benefits of splitting a large dataset into multiple files?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db357d48",
   "metadata": {},
   "source": [
    "It requires less memory. Since you train the network using fewer samples, the overall training procedure requires less memory. That's especially important if you are not able to fit the whole dataset in your machine's memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e8b34",
   "metadata": {},
   "source": [
    "### 3.\tDuring training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d140b886",
   "metadata": {},
   "source": [
    "A CPU bottleneck occurs when the GPU resource is under utilized as a result of one, or more of the CPUs, having reached maximum utilization. In this situation, the GPU will be partially idle while it waits for the CPU to pass in training data. This is an undesired state. Being that the GPU is, typically, the most expensive resource in the system, your goal should always be to maximize its utilization. Without getting into too many technical details, a CPU bottleneck generally occurs when the ratio between the “amount” of data pre-processing, which is performed on the CPU, and the “amount” of compute performed by the model on the GPU, is greater that the ratio between the overall CPU compute capacity and the overall GPU compute capacity.\n",
    "\n",
    "\n",
    "`Identifying the Bottleneck`\n",
    "\n",
    "- The first thing to check is the system resource utilization. \n",
    "\n",
    "- TensorFlow Profiler: The built in TensorFlow profiler includes a wealth of performance analytics, and in particular tools for analyzing the performance of the input pipeline. You can view using TensorBoard by installing the TensorBoard profile plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b2da18",
   "metadata": {},
   "source": [
    "`Remedies are`\n",
    "\n",
    "- `Pipeline Tuning `:\n",
    "Often times, some small tweaks to the input pipeline setup, could reduce the performance overhead. Here are just a few things you could try:\n",
    "\n",
    "   1) If you have multiple dataset map functions that are relatively small, consider grouping them into a single map function.\n",
    "   \n",
    "   2) Conversely, if you have a dataset map function that is very large, consider breaking it up into two or more smaller functions in order to better utilize the built in parallel call support.\n",
    "   \n",
    "   3) Look for operations that could be applied post batching rather than per record. (In our example, the blur function could, theoretically, be applied on training batches, but since it is typically applied in a random fashion, we will leave it per record.)\n",
    "\n",
    "   4) Use low precision types wherever possible. Postpone casting to higher precision to the end of the pipeline.\n",
    "\n",
    "   5) If your pipeline includes tf.numpy_function or tf.py_function, consider using TensorFlow primitives instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e4bbfe",
   "metadata": {},
   "source": [
    "- `CPU optimizations and extensions` : Make sure that your TensorFlow binaries were configured (and compiled) to take full advantage of your CPU, and CPU extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2209dd92",
   "metadata": {},
   "source": [
    "- `CPU Load Balancing`:   One thing that you could try, is to improve the load balancing between the CPUs so that the overall CPU utilization increases. You could try this by using the tf.config.set_logical_device_configuration API to separate the CPU compute into multiple logical devices, and the tf.device API to specify where each operation should be run. You can also try to improve the load balancing by playing around with different options for the num_parallel_calls argument of the tf.data.Dataset.mapfunction, (instead of relying on TensorFlow’s autotune feature)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7661dc",
   "metadata": {},
   "source": [
    "- `Offload Operations to the GPU`: The next option to consider is to modify the load balancing between the CPU and the GPU, by moving some of the preprocessing operations onto the GPU. The downside to this approach is that we are almost certain to increase the runtime of a GPU step. Also, since we are increasing the size of the computation graph that is running on the GPU, we may need to free up some GPU memory by running with a smaller batch size. As a result, it is highly unlikely that we will be able to achieve the target throughput we calculated above. But if it reduces the overall train time, then it is totally worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b140478",
   "metadata": {},
   "source": [
    "- `Postpone Preprocessing Operations to GPU`: In most cases, the best way to offload from the CPU, is by moving operations that are performed at the end of the preprocessing pipeline unto the GPU. By targeting these “tail” operations, rather than operations in the middle of the pipeline, we avoid the overhead of data transfers between the GPU and the CPU. If the “tail” operations are performed on the model input, we can place them at the head of the model. If they are performed on label data, we can modify our loss function to perform these operations before applying the actual loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b51788",
   "metadata": {},
   "source": [
    "### 4.\tCan you save any binary data to a TFRecord file, or only serialized protocol buffers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6788c2",
   "metadata": {},
   "source": [
    "`Why we need to convert binary files to TFRecord`\n",
    "\n",
    "The most efficient format for TensorFlow training is TensorFlow records. This is a protobuf format that makes it possible for the training program to buffer, prefetch, and parallelize the reading of records. So, a good first step for machine learning is to convert your industry-specific binary format files into TensorFlow records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888466dc",
   "metadata": {},
   "source": [
    "A TensorFlow record can only hold features which are bytes, int64, floats. Either the data has to be converted into these data types or the data itself has to be in this data format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6708d98",
   "metadata": {},
   "source": [
    "### 5.\tWhy would you go through the hassle of converting all your data to the Example protobuf format? Why not use your own protobuf definition?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0643a538",
   "metadata": {},
   "source": [
    "We recommend you use Protobuf when: You need fast serialisation/deserialisation.Google developed it with the goal to provide a better way, compared to XML, to make systems communicate. Protocol Buffers are widely used at Google for storing and interchanging all kinds of structured information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41283be6",
   "metadata": {},
   "source": [
    "### 6.\tWhen using TFRecords, when would you want to activate compression? Why not do it systematically?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce768a",
   "metadata": {},
   "source": [
    "We need to activate compression before serialization of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2206d186",
   "metadata": {},
   "source": [
    "### 7.\tData can be preprocessed directly when writing the data files, or within the tf.data pipeline, or in preprocessing layers within your model, or using TF Transform. Can you list a few pros and cons of each option?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb3aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
