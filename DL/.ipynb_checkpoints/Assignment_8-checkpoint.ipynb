{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f62e7fa",
   "metadata": {},
   "source": [
    "### 1.\tWhat are the pros and cons of using a stateful RNN versus a stateless RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6660d9f",
   "metadata": {},
   "source": [
    "`Stateful RNN:`\n",
    "\n",
    "RNNs can be stateful, which means that they can maintain state across batches during training. That is, the hidden state computed for a batch of training data will be used as the initial hidden state for the next batch of training data. The benefits of using stateful RNNs are smaller network sizes and/or lower training times.\n",
    "\n",
    "`Stateless RNN`\n",
    "\n",
    "Stateless RNN, which means the network's memory is reset for each batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6482a098",
   "metadata": {},
   "source": [
    "### 2.\tWhy do people use Encoder–Decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231c9bf3",
   "metadata": {},
   "source": [
    "The encoder-decoder architecture can handle inputs and outputs that are both variable-length sequences, thus is suitable for sequence transduction problems such as machine translation. The encoder takes a variable-length sequence as the input and transforms it into a state with a fixed shape.\n",
    "\n",
    "\n",
    "When we use an encoder, the meaning of the sentence will be stored and represented by a vector.We can’t directly convert a sentence in French to its English translation because we would lose the context by the Decoder.\n",
    "\n",
    "Indeed, if we translate directly, we would in fact translate word by word. Without caring about the global meaning of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8c18da",
   "metadata": {},
   "source": [
    "### 3.\tHow can you deal with variable-length input sequences? What about variable-length output sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d618b7",
   "metadata": {},
   "source": [
    "We can handle variable length input to RNN by Padding. not all the sequences have the same length, as we can say naturally some of the sequences are long in lengths and some are short. Where we know that we need to have the inputs with the same size. So we add the zeros at the end of the sequence to make the samples in the same size.\n",
    "\n",
    "We dont do padding of the output sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a5e144",
   "metadata": {},
   "source": [
    "### 4.\tWhat is beam search and why would you use it? What tool can you use to implement it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e425664",
   "metadata": {},
   "source": [
    "Beam search is the most popular search strategy for the sequence to sequence Deep NLP algorithms like Neural Machine Translation, Image captioning, Chatbots, etc. Beam search considers multiple best options based on beamwidth using conditional probability, which is better than the sub-optimal Greedy search.\n",
    "\n",
    "A beam search is most often used to maintain tractability in large systems with insufficient memory to store the entire search tree. For example, It has been used in many machine translation systems. Each part is processed to select the best translation, and many different ways of translating the words appear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2346ec84",
   "metadata": {},
   "source": [
    "### 5.\tWhat is an attention mechanism? How does it help?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4735e22d",
   "metadata": {},
   "source": [
    "Attention Mechanism is also an attempt to implement the same action of selectively concentrating on a few relevant things, while ignoring others in deep neural networks.\n",
    "\n",
    "The advantages of attention is its ability to identify the information in an input most pertinent to accomplishing a task, increasing performance especially in natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b32dd4",
   "metadata": {},
   "source": [
    "### 6.\tWhat is the most important layer in the Transformer architecture? What is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2560ee79",
   "metadata": {},
   "source": [
    "The most important part here is the “Residual Connections” around the layers. This is very important in retaining the position related information which we are adding to the input representation/embedding across the network. The network displayed catastrophic results on removing the Residual Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b89d7da",
   "metadata": {},
   "source": [
    "### 7.\tWhen would you need to use sampled softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a940c",
   "metadata": {},
   "source": [
    "If we were using full softmax cross entropy, we would compute all scores with a full matrix multiplication. With sampled softmax we can save computation and memory by selecting only the rows of that are needed for the loss. One optional tweak is to share noise samples between elements of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc31370",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
