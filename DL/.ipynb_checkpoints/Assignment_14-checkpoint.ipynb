{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "029f8bab",
   "metadata": {},
   "source": [
    "### 1.\tIs it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f8ab5e",
   "metadata": {},
   "source": [
    "Initializing all the weights with same value leads the neurons to learn the same features during training. In fact, any constant initialization scheme will perform very poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9391b979",
   "metadata": {},
   "source": [
    "### 2.\tIs it okay to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64323a4e",
   "metadata": {},
   "source": [
    "It is important to note that setting biases to 0 will not create any problems as non-zero weights take care of breaking the symmetry and even if bias is 0, the values in every neuron will still be different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97527644",
   "metadata": {},
   "source": [
    "### 3.\tName three advantages of the ELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af0d91",
   "metadata": {},
   "source": [
    "1) Similar to ReLUs, SELUs enable deep neural networks since there is no problem with vanishing gradients.\n",
    "\n",
    "2) In contrast to ReLUs, SELUs cannot die.\n",
    "\n",
    "3) SELUs on their own learn faster and better than other activation functions, even if they are combined with batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8effb04",
   "metadata": {},
   "source": [
    "### 4.\tIn which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b5e413",
   "metadata": {},
   "source": [
    "`Sigmoid/ logistic:`\n",
    "    \n",
    "- It is commonly used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice because of its range.\n",
    "\n",
    "- The function is differentiable and provides a smooth gradient, i.e., preventing jumps in output values.\n",
    "\n",
    "\n",
    "`tanh`: \n",
    "\n",
    "- The output of the tanh activation function is Zero centered; hence we can easily map the output values as strongly negative, neutral, or strongly positive.\n",
    "\n",
    "- Usually used in hidden layers of a neural network as its values lie between -1 to; therefore, the mean for the hidden layer comes out to be 0 or very close to it. It helps in centering the data and makes learning for the next layer much easier.\n",
    "\n",
    "`softmax`\n",
    "\n",
    "- The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability\n",
    "\n",
    "\n",
    "`Relu`\n",
    "\n",
    "- Since only a certain number of neurons are activated, the ReLU function is far more computationally efficient when compared to the sigmoid and tanh functions.\n",
    "\n",
    "- ReLU accelerates the convergence of gradient descent towards the global minimum of the loss function due to its linear, non-saturating property. Hence Rleu is commenly used in hidden layers\n",
    "\n",
    "`Leaky relu`\n",
    "\n",
    "- Advanages are similar to Relu activation function, and Dying relu problem is comparitvely less to Leaky relu\n",
    "\n",
    "- Leaky relu is used in Hidden layers commonly.\n",
    "\n",
    "`SELU (Scaled Exponential Linear Unit)`\n",
    "\n",
    "- Internal normalization is faster than external normalization, which means the network converges faster. Used in Hideen layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68a08fc",
   "metadata": {},
   "source": [
    "### 5.\tWhat may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b087220f",
   "metadata": {},
   "source": [
    "If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer, then the algorithm will likely pick up a lot of speed, hopefully roughly toward the global minimum, but then it will shoot right past the minimum, due to its momentum. Then it will slow down and come back, accelerate again, overshoot again, and so on. It may oscillate this way many times before converging, so overall it will take much longer to converge than with a smaller momentum value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af39283",
   "metadata": {},
   "source": [
    "### 6.\tName three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b654dae",
   "metadata": {},
   "source": [
    "Train the model normally, then zero out tiny weights.\n",
    "For more sparsity, you can\n",
    "apply ℓ1 regularization (Lasso) during training, which pushes the optimizer to zero out as many weights as it can.\n",
    "A third option is to combine ℓ1 regularization with dual averaging, using TensorFlow's FTRLOptimizer class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c2d757",
   "metadata": {},
   "source": [
    "### 7.\tDoes dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bef099",
   "metadata": {},
   "source": [
    "Logically, by omitting at each iteration neurons with a dropout, those omitted on an iteration are not updated during the backpropagation. They do not exist. So the training phase is slowed down. However, applying dropout to a neural network typically increases the training time.\n",
    "\n",
    "Dropout will not affect during Inferencing, Since all neurons will be actively participating during inferencing\n",
    "\n",
    "Monte Carlo Dropout boils down to training a neural network with the regular dropout and keeping it switched on at inference time. This way, we can generate multiple different predictions for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb0a85e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
