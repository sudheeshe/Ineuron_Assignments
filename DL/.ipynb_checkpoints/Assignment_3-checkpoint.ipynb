{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecea5b1e",
   "metadata": {},
   "source": [
    "### 1.\tIs it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d77126",
   "metadata": {},
   "source": [
    "Initializing all the weights with same value leads the neurons to learn the same features during training. In fact, any constant initialization scheme will perform very poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc14423",
   "metadata": {},
   "source": [
    "### 2.\tIs it OK to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba35015",
   "metadata": {},
   "source": [
    "It is important to note that setting biases to 0 will not create any problems as non-zero weights take care of breaking the symmetry and even if bias is 0, the values in every neuron will still be different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac27f5",
   "metadata": {},
   "source": [
    "### 3.\tName three advantages of the SELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d747fc6a",
   "metadata": {},
   "source": [
    "1) Similar to ReLUs, SELUs enable deep neural networks since there is no problem with vanishing gradients.\n",
    "\n",
    "2) In contrast to ReLUs, SELUs cannot die.\n",
    "\n",
    "3) SELUs on their own learn faster and better than other activation functions, even if they are combined with batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3545ced2",
   "metadata": {},
   "source": [
    "### 4.\tIn which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eddaec",
   "metadata": {},
   "source": [
    "`Sigmoid/ logistic:`\n",
    "    \n",
    "- It is commonly used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice because of its range.\n",
    "\n",
    "- The function is differentiable and provides a smooth gradient, i.e., preventing jumps in output values.\n",
    "\n",
    "\n",
    "`tanh`: \n",
    "\n",
    "- The output of the tanh activation function is Zero centered; hence we can easily map the output values as strongly negative, neutral, or strongly positive.\n",
    "\n",
    "- Usually used in hidden layers of a neural network as its values lie between -1 to; therefore, the mean for the hidden layer comes out to be 0 or very close to it. It helps in centering the data and makes learning for the next layer much easier.\n",
    "\n",
    "`softmax`\n",
    "\n",
    "- The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability\n",
    "\n",
    "\n",
    "`Relu`\n",
    "\n",
    "- Since only a certain number of neurons are activated, the ReLU function is far more computationally efficient when compared to the sigmoid and tanh functions.\n",
    "\n",
    "- ReLU accelerates the convergence of gradient descent towards the global minimum of the loss function due to its linear, non-saturating property. Hence Rleu is commenly used in hidden layers\n",
    "\n",
    "`Leaky relu`\n",
    "\n",
    "- Advanages are similar to Relu activation function, and Dying relu problem is comparitvely less to Leaky relu\n",
    "\n",
    "- Leaky relu is used in Hidden layers commonly.\n",
    "\n",
    "`SELU (Scaled Exponential Linear Unit)`\n",
    "\n",
    "- Internal normalization is faster than external normalization, which means the network converges faster. Used in Hideen layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475f0d85",
   "metadata": {},
   "source": [
    "### 5.\tWhat may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a3a261",
   "metadata": {},
   "source": [
    "then the algorithm will likely pick up a lot of speed, hopefully moving roughly toward the global minimum, but its momentum will carry it right past the minimum which is called as overshooting. And the SGD will never converge to global minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e90e69",
   "metadata": {},
   "source": [
    "### 6.\tName three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b78d0b1",
   "metadata": {},
   "source": [
    "Train the model normally, then zero out tiny weights.\n",
    "For more sparsity, you can\n",
    "apply ℓ1 regularization (Lasso) during training, which pushes the optimizer to zero out as many weights as it can.\n",
    "A third option is to combine ℓ1 regularization with dual averaging, using TensorFlow's FTRLOptimizer class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0f0a3f",
   "metadata": {},
   "source": [
    "### 7.\tDoes dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb51ace5",
   "metadata": {},
   "source": [
    "Logically, by omitting at each iteration neurons with a dropout, those omitted on an iteration are not updated during the backpropagation. They do not exist. So the training phase is slowed down. However, applying dropout to a neural network typically increases the training time.\n",
    "\n",
    "Dropout will not affect during Inferencing, Since all neurons will be actively participating during inferencing\n",
    "\n",
    "Monte Carlo Dropout boils down to training a neural network with the regular dropout and keeping it switched on at inference time. This way, we can generate multiple different predictions for each instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0738b584",
   "metadata": {},
   "source": [
    "### 8.\tPractice training a deep neural network on the CIFAR10 image dataset:\n",
    "\n",
    "- a.\tBuild a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.\n",
    "\n",
    "- b.\tUsing Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
    "\n",
    "- c.\tNow try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "\n",
    "- d.\tTry replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "\n",
    "- e.\tTry regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d1c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models, activations\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i])\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(class_names[train_labels[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e4c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.HeNormal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2005effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=50, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b3a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87723e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='nadam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=50, \n",
    "                    validation_data=(test_images, test_labels), callbacks = early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e78b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec1dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.LecunNormal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6d114",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(100, activation = 'elu', kernel_initializer=initializer))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='nadam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=50, \n",
    "                    validation_data=(test_images, test_labels), callbacks = early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3324c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d5a508",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.Dense(10, activation = 'softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='nadam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=50, \n",
    "                    validation_data=(test_images, test_labels), callbacks = early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b3fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788338ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(100, activation = 'selu', kernel_initializer=initializer))\n",
    "model.add(layers.AlphaDropout(10))\n",
    "model.add(layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='nadam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=50, \n",
    "                    validation_data=(test_images, test_labels), callbacks = early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e572dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
